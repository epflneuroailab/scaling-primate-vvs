# Model
arch: deit3_small_patch16_224
drop_path_rate: 0.0
device: 'gpu'
layer_scale_init_value: 1e-4
compile_mode: 'None'

# Optimizer & LR scheme
loss_fn: 'cross_entropy'
max_duration: '100ep'
batch_size: 512
opt: 'adamw'
lr: 3e-3
min_lr: 1e-5
lr_scheduler: 'cosineannealinglrwithwarmup'
lr_warmup_duration: '5ep'
weight_decay: 0.02
opt_betas: [0.9, 0.999]
opt_eps: 1e-8
clip_grad: 1.0
label_smoothing: 0.0
accumulation_steps: 1
target_batch_size: 2048


# Data
data_path: '/mnt/scratch/akgokce/datasets/imagenet/'
workers: 10
pin_memory: True
train_samples: 0
seed: 0

# Resizing
val_resize_size: 256
val_crop_size: 224
train_crop_size: 224

# Misc
disable_progress_bar: False
lr_monitor: False
log_interval: "1ba"

# Logging
eval_interval: '1ep'
wandb_project: 'scalingLaws'
wandb_entity: 'akgokce'
save_dir: '/mnt/scratch/akgokce/brain/outputs2/'
run_name: deit3_small_imagenet_full-v1